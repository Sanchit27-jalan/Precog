{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-_v-gnGDFYv",
        "outputId": "a3dbd3b2-d3c2-43a1-b771-753dba1c9907"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow --break-system-packages\n",
        "# !pip install nltk --break-system-packages\n",
        "# !pip install numpy --break-system-packages\n",
        "# !pip install pandas --break-system-packages\n",
        "# !pip install gensim --break-system-packages\n",
        "# !pip install tqdm --break-system-packages\n",
        "# !pip install scipy --break-system-packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_0Kpxg8Gmni",
        "outputId": "cc792b38-6220-437c-c4fc-a58f120d04df"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import brown\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "from math import sqrt\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SZdz1qhGp6c"
      },
      "outputs": [],
      "source": [
        "def get_cosine(vector1, vector2):\n",
        "    intersection = min(len(vector1), len(vector2))\n",
        "    numerator = sum(vector1[i] * vector2[i] for i in range(intersection))\n",
        "    sum1 = sum(vector1[i] ** 2 for i in range(len(vector1)))\n",
        "    sum2 = sum(vector2[i] ** 2 for i in range(len(vector2)))\n",
        "    denominator = sqrt(sum1) * sqrt(sum2)\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "def reduce_dimensionality(original_vector, new_length):\n",
        "    # Assuming original_vector is a 1D NumPy array\n",
        "    original_vector = original_vector.reshape(1, -1)  # Convert to a 2D array\n",
        "    pca = PCA(n_components=new_length)\n",
        "    reduced_vector = pca.fit_transform(original_vector)\n",
        "    reduced_vector = reduced_vector.flatten()  # Convert back to a 1D array\n",
        "\n",
        "    return reduced_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWjBAuo9GxRi",
        "outputId": "495d258d-3221-4eb1-8256-951254f600ea"
      },
      "outputs": [],
      "source": [
        "sentences = brown.sents()\n",
        "\n",
        "tokenized_sentences = [word_tokenize(' '.join(sentence)) for sentence in sentences]\n",
        "\n",
        "model = Word2Vec(sentences=tokenized_sentences, vector_size=300, window=5, min_count=1, workers=4)\n",
        "import pandas as pd\n",
        "simlex_subset = pd.read_csv('./SimLex-999.txt', sep='\\t')\n",
        "word1_list = simlex_subset['word1'].tolist()\n",
        "word2_list = simlex_subset['word2'].tolist()\n",
        "predictions=[]\n",
        "for i in tqdm(range(0,len(word1_list))):\n",
        "    vector1 = model.wv[word1_list[i]] if word1_list[i] in model.wv else np.zeros(model.vector_size)\n",
        "    vector2 = model.wv[word2_list[i]] if word2_list[i] in model.wv else np.zeros(model.vector_size)\n",
        "    if np.any(np.isnan(vector1)) or np.any(np.isnan(vector2)) or np.all(vector1 == 0) or np.all(vector2 == 0):\n",
        "        similarity = 0\n",
        "    else:\n",
        "        similarity = get_cosine(vector1,vector2)\n",
        "\n",
        "    predictions.append(similarity)\n",
        "ground_truth_scores = simlex_subset['SimLex999'].values\n",
        "correlation, _ = spearmanr(predictions, ground_truth_scores)\n",
        "print(f'Spearman Correlation: {correlation*100}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
